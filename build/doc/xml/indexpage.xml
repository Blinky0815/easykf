<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.6">
  <compounddef id="indexpage" kind="page">
    <compoundname>index</compoundname>
    <title>Kalman Filters</title>
    <detaileddescription>
<sect1 id="index_1intro_sec">
<title>Introduction</title>
<para>Two algorithms are implemented and all of them taken from the PhD of Van Der Merwe &quot;Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space Models&quot; :<linebreak/>
<itemizedlist>
<listitem><para>UKF for parameter estimation, Algorithm 6, p. 93<linebreak/>
</para></listitem><listitem><para>UKF for state or joint estimation, additive noise case, Algorithm 8, p. 108 <linebreak/>
</para></listitem></itemizedlist>
</para><sect2 id="index_1usage">
<title>Usage</title>
<para>To use the library, you simply need to :<itemizedlist>
<listitem><para>define a parameter and state structure, which depends on the algorithm you use : ukf_*_param, ukf_*_state</para></listitem><listitem><para>call the proper function initializing the state : ukf_*_init()</para></listitem><listitem><para>iterate with the proper function by providing a sample <formula id="0">$ (x_i, y_i) $</formula> : ukf_*_iterate()</para></listitem><listitem><para>free the memory : ukf_*_free() To use these functions, you simply need to define you evolution/observation functions, provided to the ukf_*_iterate functions as well as the samples.<linebreak/>
</para><para><bold>Warning :</bold> Be sure to always use gsl_vector_get and gsl_vector_set in your evolution/observation functions, never access the fiels of the vectors with the data array of the gsl_vectors.<linebreak/>
</para></listitem></itemizedlist>
</para></sect2>
<sect2 id="index_1param_estimation">
<title>UKF for parameter estimation</title>
<para>For UKF for parameter estimation, two versions are implemented : in case of a scalar output or vectorial output. The vectorial version works also in the scalar case but is more expensive (memory and time) than the scalar version when the output is scalar.<linebreak/>
 For the <bold>scalar</bold> version :<itemizedlist>
<listitem><para>the structures to define are <ref refid="structukf_1_1parameter_1_1ukf__param" kindref="compound">ukf::parameter::ukf_param</ref> and <ref refid="structukf_1_1parameter_1_1ukf__scalar__state" kindref="compound">ukf::parameter::ukf_scalar_state</ref></para></listitem><listitem><para>the methods to initialize, iterate, evaluate and free are : <ref refid="namespaceukf_1_1parameter_1a4f5d4fb6f2a2be962a1528a40f8c9562" kindref="member">ukf::parameter::ukf_scalar_init</ref> , <ref refid="namespaceukf_1_1parameter_1ae4830e1c0b576662494d63623dcf59dc" kindref="member">ukf::parameter::ukf_scalar_iterate</ref> , <ref refid="namespaceukf_1_1parameter_1a9b3034fa2b941068c2fc0fea41cbd5fb" kindref="member">ukf::parameter::ukf_scalar_evaluate</ref> , <ref refid="namespaceukf_1_1parameter_1a18d91f67e6de67928a223b50950ffdae" kindref="member">ukf::parameter::ukf_scalar_free&lt;BR&gt;</ref></para></listitem></itemizedlist>
</para><para>Examples using the scalar version : <linebreak/>
<itemizedlist>
<listitem><para>Training a simple MPL on the XOR problem : example-001.cc</para></listitem><listitem><para>Training a MLP on the extended XOR : example-002.cc</para></listitem><listitem><para>Training a RBF for fitting a sinc function : example-003.cc</para></listitem><listitem><para>Finding the minimum of the Rosenbrock banana function : example-006.cc</para></listitem></itemizedlist>
</para><para>For the <bold>vectorial</bold> version :<itemizedlist>
<listitem><para>the structures to define are <ref refid="structukf_1_1parameter_1_1ukf__param" kindref="compound">ukf::parameter::ukf_param</ref> and <ref refid="structukf_1_1parameter_1_1ukf__state" kindref="compound">ukf::parameter::ukf_state</ref></para></listitem><listitem><para>the methods to initialize, iterate, evaluate and free are : <ref refid="namespaceukf_1_1parameter_1ad83710f24d64d3b511ff60aa4fc90f03" kindref="member">ukf::parameter::ukf_init</ref> , <ref refid="namespaceukf_1_1parameter_1aa7af8ceffba2f9288b2d7b86ed57159f" kindref="member">ukf::parameter::ukf_iterate</ref> , <ref refid="namespaceukf_1_1parameter_1aaf6c5a1c836abd3dd07c27a3eec2d097" kindref="member">ukf::parameter::ukf_evaluate</ref> , <ref refid="namespaceukf_1_1parameter_1a3edd781b5248176b7404889c96e6a962" kindref="member">ukf::parameter::ukf_free&lt;BR&gt;</ref></para></listitem></itemizedlist>
</para><para>Examples using the vectorial version : <linebreak/>
<itemizedlist>
<listitem><para>Training a 2-2-3 MLP to learn the OR, AND, XOR functions : example-004.cc</para></listitem><listitem><para>Training a 2-12-2 MLP to learn the Mackay Robot arm data : example-005.cc</para></listitem></itemizedlist>
</para></sect2>
<sect2 id="index_1joint_ukf">
<title>Joint UKF</title>
<para>The Joint UKF tries to estimate both the state and the parameters of a system. The structures/methods related to Joint UKF are :<itemizedlist>
<listitem><para><ref refid="structukf_1_1state_1_1ukf__param" kindref="compound">ukf::state::ukf_param</ref> and <ref refid="structukf_1_1state_1_1ukf__state" kindref="compound">ukf::state::ukf_state</ref> for the parameters and the state representations</para></listitem><listitem><para><ref refid="namespaceukf_1_1state_1ad6760ca25736be80b2b9e9f9cbe9f8e8" kindref="member">ukf::state::ukf_init</ref>, <ref refid="namespaceukf_1_1state_1a07694a4f76574d87caf4b1e756ccae00" kindref="member">ukf::state::ukf_free</ref>, <ref refid="namespaceukf_1_1state_1a3a0665674b96f3e979ca39d9d4583f43" kindref="member">ukf::state::ukf_iterate</ref>, <ref refid="namespaceukf_1_1state_1a2b40e84fd304480317cc6f0736a058fc" kindref="member">ukf::state::ukf_evaluate</ref> for respectively initializing the structures, freeing the memory, iterating on one sample and evaluating the observation from the sigma points.<linebreak/>
 To see how to use Joint UKF, have a look to the example example-007.cc where we seek the parameters and state of a Lorentz attractor.</para></listitem></itemizedlist>
</para></sect2>
</sect1>
<sect1 id="index_1install_sec">
<title>Installation and running</title>
<sect2 id="index_1tools_subsec">
<title>Requirements:</title>
<para>In addition to a g++ compiler with the standard libraries, you also need to install :<itemizedlist>
<listitem><para>GSL (Gnu Scientific Library), available here : <ulink url="http://www.gnu.org/software/gsl/">http://www.gnu.org/software/gsl/</ulink></para></listitem><listitem><para>cmake for compilation</para></listitem></itemizedlist>
</para></sect2>
<sect2 id="index_1compilation">
<title>Compilation, Installation</title>
<para>The installation follows the standard, for example on Linux : mkdir build cd build cmake .. -G&quot;Unix Makefiles&quot; -DCMAKE_INSTALL_PREFIX=&lt;the prefix=&quot;&quot; where=&quot;&quot; you=&quot;&quot; want=&quot;&quot; the=&quot;&quot; files=&quot;&quot; to=&quot;&quot; be=&quot;&quot; installed&gt;=&quot;&quot;&gt; make make install</para><para>It will compile the library, the examples, the documentation and install them.</para></sect2>
</sect1>
<sect1 id="index_1example">
<title>Example outputs</title>
<sect2 id="index_1example1">
<title>Example 1 : Learning XOR with a 2-2-1 MLP</title>
<para>Running (maybe several times if falling on a local minima) example-001-xor, you should get the following classification :</para><para><image type="html" name="example-001.png">XOR classification</image>
 An example set of learned parameters is :</para><para>x[0] <ndash/> (9.89157) <ndash/>&gt; y[0] <linebreak/>
 x[1] <ndash/> (4.18644) <ndash/>&gt; y[0]<linebreak/>
 Bias y[0] : 8.22042<linebreak/>
</para><para>x[0] <ndash/> (10.7715) <ndash/>&gt; y[1]<linebreak/>
 x[1] <ndash/> (4.18047) <ndash/>&gt; y[1]<linebreak/>
 Bias y[1] : -8.70185<linebreak/>
</para><para>y[0] <ndash/> (6.9837) <ndash/>&gt; z<linebreak/>
 y[1] <ndash/> (-6.83324) <ndash/>&gt; z<linebreak/>
 Bias z : -3.89682<linebreak/>
</para><para>The transfer function is a sigmoid : <formula id="1">$ f(x) = \frac{2}{1 + exp(-x)}-1$</formula></para></sect2>
<sect2 id="index_1example2">
<title>Example 2 : Learning the extended XOR with a 2-12-1 MLP and a parametrized transfer function</title>
<para>Here we use a 2-12-1 MLP, with a sigmoidal transfer function, to learn the extended XOR problem. The transfer function has the shape : <formula id="2">$f(x) = \frac{1}{1.0 + exp(-x)}$</formula></para><para>The classification should look like this :</para><para><image type="html" name="example-002.png">Extended XOR classification</image>
 </para></sect2>
<sect2 id="index_1example3">
<title>Example 3 : Approximating the sinc function with a Radial Basis Function network</title>
<para>In this example, we use a RBF network with 10 kernels to approximate the sinc function on [-5.0,5.0] To make the life easier for the algorithm, we evenly spread the centers of the gaussians on [-5.0, 5.0].</para><para>The results are saved in &apos;example-003.data&apos;, the first column contains the x-position, the second column the result given by the trained RBF and the last column the value of sinc(x)</para><para><image type="html" name="example-003.png">RBF learning the sinc function</image>
 </para></sect2>
<sect2 id="index_1example4">
<title>Example 4 : Using a 2-2-3 MLP to learn three boolean functions : XOR, AND, OR</title>
</sect2>
<sect2 id="index_1example5">
<title>Example 5 : Using a 2-12-2 MLP to learn the Mackay-robot arm problem</title>
<para>In this example, we learn the two outputs (x,y) from the inputs (theta, phi) of the Mackay-robot arm dataset. For this we train a 2-12-2 MLP with a parametrized sigmoidal transfer function.</para><para><image type="html" name="example-005-x.png">Learning the x-component</image>
 <image type="html" name="example-005-y.png">Learning the y-component</image>
 </para></sect2>
<sect2 id="index_1example6">
<title>Example 6 : Finding the minimum of the Rosenbrock banana function</title>
<para>We use here UKF for parameter estimation to find the minimum of the Rosenbrock banana function : <formula id="3">$ f(x,y) = (1 - x)^2 + 100 ( y - x^2)^2 $</formula><linebreak/>
</para><para><image type="html" name="example-006.png">Minimisation of the Rosenbrock banana function</image>
 </para></sect2>
<sect2 id="index_1example7">
<title>Example 7 : Finding the parameters of a Lorentz attractor</title>
<para>In this example, we try to find the parameters (initial condition, evolution parameters) of a noisy lorentz attractor. The dynamic of the lorentz attractor is defined by the three equations :</para><para><formula id="4">$ \frac{dx}{dt} = \sigma ( y - x ) $</formula> <linebreak/>
 <formula id="5">$ \frac{dy}{dt} = x (\rho - z) - y $</formula> <linebreak/>
 <formula id="6">$ \frac{dz}{dt} = xy - \beta z $</formula> <linebreak/>
 While observing a noisy trajectory of such a Lorentz attractor, the algorithm tries to find the current state and the evolution parameters <formula id="7">$ (\sigma, \rho, \beta)$</formula>. The samples we provide are <formula id="8">$ (t_i, {x(t_i), y(t_i), z(t_i)})$</formula>.</para><para>To clearly see how UKF catches the true state, we initialized the estimated state of UKF to -15, -15 , -15<linebreak/>
</para><para><image type="html" name="example-007-rms.png">Learning RMS</image>
 <image type="html" name="example-007.png">Estimated state with the true state and its noisy observation</image>
 <linebreak/>
<linebreak/>
 </para></sect2>
</sect1>
    </detaileddescription>
  </compounddef>
</doxygen>
